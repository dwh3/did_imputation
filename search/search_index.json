{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"didimpute Documentation Overview Welcome to the didimpute project site. This documentation provides a high-level entry point for economists and applied researchers adopting the imputation-based DID estimator implemented in Python. Getting Started Install the package via pip install didimpute or add optional extras such as plotting ( pip install \"didimpute[plot]\" ). Review the API usage patterns in the repository README (ensure the latest copy is restored locally) and the examples under examples/ . Key References Parity Validation \u2014 see the dedicated page in this site navigation to understand how Python outputs are compared with the Stata benchmark. Specification & Architecture \u2014 consult docs/SPEC.md and ARCHITECTURE.md in the repository for deeper implementation details. Testing Strategy \u2014 TEST_STRATEGY.md documents deterministic seeds, tolerance thresholds, and regression guardrails. Helpful Links Source: https://github.com/dwh3/did_imputation Issues & Support: https://github.com/dwh3/did_imputation/issues Package README: ensure the repository README.md is available for installation instructions, citation guidance, and reproducibility notes.","title":"Overview"},{"location":"index.html#didimpute-documentation-overview","text":"Welcome to the didimpute project site. This documentation provides a high-level entry point for economists and applied researchers adopting the imputation-based DID estimator implemented in Python.","title":"didimpute Documentation Overview"},{"location":"index.html#getting-started","text":"Install the package via pip install didimpute or add optional extras such as plotting ( pip install \"didimpute[plot]\" ). Review the API usage patterns in the repository README (ensure the latest copy is restored locally) and the examples under examples/ .","title":"Getting Started"},{"location":"index.html#key-references","text":"Parity Validation \u2014 see the dedicated page in this site navigation to understand how Python outputs are compared with the Stata benchmark. Specification & Architecture \u2014 consult docs/SPEC.md and ARCHITECTURE.md in the repository for deeper implementation details. Testing Strategy \u2014 TEST_STRATEGY.md documents deterministic seeds, tolerance thresholds, and regression guardrails.","title":"Key References"},{"location":"index.html#helpful-links","text":"Source: https://github.com/dwh3/did_imputation Issues & Support: https://github.com/dwh3/did_imputation/issues Package README: ensure the repository README.md is available for installation instructions, citation guidance, and reproducibility notes.","title":"Helpful Links"},{"location":"DOCS_DEPLOY_STATUS.html","text":"Docs Deploy Status \u2014 $(git rev-parse --short HEAD) Updated mkdocs.yml with: site_url: https://dwh3.github.io/did_imputation/ repo_url: https://github.com/dwh3/did_imputation use_directory_urls: false Navigation pointing to index.md and PARITY.md . Documentation files present in docs/ : index.md , PARITY.md , SPEC.md , RELEASE_STATUS.md . Build command: .venv/Scripts/python.exe -m mkdocs build --strict (status: \u2705). Deploy command: .venv/Scripts/python.exe -m mkdocs gh-deploy --clean --message \"docs: fix project-site base path and links\" (status: \u2705). Published site: https://dwh3.github.io/did_imputation/ . \ud83d\udccc Maintainer reminder: In GitHub \u2192 Settings \u2192 Pages , ensure Source = gh-pages (root) and allow a few minutes for CDN cache invalidation before testing navigation.","title":"Docs Deploy Status \u2014 $(git rev-parse --short HEAD)"},{"location":"DOCS_DEPLOY_STATUS.html#docs-deploy-status-git-rev-parse-short-head","text":"Updated mkdocs.yml with: site_url: https://dwh3.github.io/did_imputation/ repo_url: https://github.com/dwh3/did_imputation use_directory_urls: false Navigation pointing to index.md and PARITY.md . Documentation files present in docs/ : index.md , PARITY.md , SPEC.md , RELEASE_STATUS.md . Build command: .venv/Scripts/python.exe -m mkdocs build --strict (status: \u2705). Deploy command: .venv/Scripts/python.exe -m mkdocs gh-deploy --clean --message \"docs: fix project-site base path and links\" (status: \u2705). Published site: https://dwh3.github.io/did_imputation/ . \ud83d\udccc Maintainer reminder: In GitHub \u2192 Settings \u2192 Pages , ensure Source = gh-pages (root) and allow a few minutes for CDN cache invalidation before testing navigation.","title":"Docs Deploy Status \u2014 $(git rev-parse --short HEAD)"},{"location":"PARITY.html","text":"Parity Summary (Python \u2194 Stata) Motivation Imputation-based DID estimators provide unbiased event-study trajectories under staggered adoption when counterfactual outcomes are constructed from untreated cohorts. didimpute reproduces the workflow economists know from Stata while exposing a Pythonic API suitable for reproducible pipelines. To instill confidence that the port preserves the original estimator\u2019s behavior, we validate against the canonical Stata implementation across seeded data-generating processes (DGPs). What \u201cParity\u201d Means Here Event-time estimates ( \u03c4_k ) : we align each treated event time k between Stata and Python outputs and compare point estimates. Standard errors : where Stata emits cluster-robust standard errors, we line them up and compute absolute deviations. Pretrend diagnostics : placebo effects for k < 0 are tabulated so that the joint Wald tests agree in rejection behavior. Coverage heuristics : we report share of estimates within user-defined tolerances (default \u22640.05 absolute difference). All comparisons run on deterministic CSV fixtures generated under seeds 100\u2013149; the harness records raw tables plus summary statistics to parity/out/ . Headline Results (latest manual run) dgpB_const_te (scheme= nobs , k=0..3): Share(|\u0394 estimate| \u2264 0.05) = 1.00; Max |\u0394| = 0.0000 dgpB_const_te (scheme= equal ): 1.00 dgpB_const_te (scheme= cohort_share ): 1.00 dgpC_pretrend (scheme= nobs , k=-3..0): 1.00 dgpA_no_treat : no overlapping treated k rows (expected) The full parity report lives at parity/out/PARITY_REPORT.md , alongside raw CSV tables for transparency. Interpreting the Report Share(|\u0394 estimate| \u2264 \u03b5) close to 1.0 indicates the estimator matches Stata within tolerance \u03b5 ; investigate outliers when the share drops. Max |\u0394| highlights worst-case deviations. Non-zero values typically signal insufficient support or failing first-stage assumptions. Pretrend rows : when Stata lacks overlapping negative event-times, the Python harness marks the test as \u201cnot run\u201d to avoid false alarms. Missing rows : if Stata omits certain k (e.g., no treated observations), expect nan /missing entries; didimpute mirrors that behavior by design. When extending the estimator or adding weight schemes, update the parity harness to cover new scenarios and re-freeze the golden regression outputs in tests/golden/ . Regression Guardrails Python goldens for canonical scenarios live in tests/golden/ . CI enforces parity by comparing fresh summaries to these goldens with tight tolerances (1e-6 absolute on estimates/SEs). Stata is optional : the parity harness can be run manually when Stata is available; CI depends only on the goldens.","title":"Parity Validation"},{"location":"PARITY.html#parity-summary-python-stata","text":"","title":"Parity Summary (Python \u2194 Stata)"},{"location":"PARITY.html#motivation","text":"Imputation-based DID estimators provide unbiased event-study trajectories under staggered adoption when counterfactual outcomes are constructed from untreated cohorts. didimpute reproduces the workflow economists know from Stata while exposing a Pythonic API suitable for reproducible pipelines. To instill confidence that the port preserves the original estimator\u2019s behavior, we validate against the canonical Stata implementation across seeded data-generating processes (DGPs).","title":"Motivation"},{"location":"PARITY.html#what-parity-means-here","text":"Event-time estimates ( \u03c4_k ) : we align each treated event time k between Stata and Python outputs and compare point estimates. Standard errors : where Stata emits cluster-robust standard errors, we line them up and compute absolute deviations. Pretrend diagnostics : placebo effects for k < 0 are tabulated so that the joint Wald tests agree in rejection behavior. Coverage heuristics : we report share of estimates within user-defined tolerances (default \u22640.05 absolute difference). All comparisons run on deterministic CSV fixtures generated under seeds 100\u2013149; the harness records raw tables plus summary statistics to parity/out/ .","title":"What \u201cParity\u201d Means Here"},{"location":"PARITY.html#headline-results-latest-manual-run","text":"dgpB_const_te (scheme= nobs , k=0..3): Share(|\u0394 estimate| \u2264 0.05) = 1.00; Max |\u0394| = 0.0000 dgpB_const_te (scheme= equal ): 1.00 dgpB_const_te (scheme= cohort_share ): 1.00 dgpC_pretrend (scheme= nobs , k=-3..0): 1.00 dgpA_no_treat : no overlapping treated k rows (expected) The full parity report lives at parity/out/PARITY_REPORT.md , alongside raw CSV tables for transparency.","title":"Headline Results (latest manual run)"},{"location":"PARITY.html#interpreting-the-report","text":"Share(|\u0394 estimate| \u2264 \u03b5) close to 1.0 indicates the estimator matches Stata within tolerance \u03b5 ; investigate outliers when the share drops. Max |\u0394| highlights worst-case deviations. Non-zero values typically signal insufficient support or failing first-stage assumptions. Pretrend rows : when Stata lacks overlapping negative event-times, the Python harness marks the test as \u201cnot run\u201d to avoid false alarms. Missing rows : if Stata omits certain k (e.g., no treated observations), expect nan /missing entries; didimpute mirrors that behavior by design. When extending the estimator or adding weight schemes, update the parity harness to cover new scenarios and re-freeze the golden regression outputs in tests/golden/ .","title":"Interpreting the Report"},{"location":"PARITY.html#regression-guardrails","text":"Python goldens for canonical scenarios live in tests/golden/ . CI enforces parity by comparing fresh summaries to these goldens with tight tolerances (1e-6 absolute on estimates/SEs). Stata is optional : the parity harness can be run manually when Stata is available; CI depends only on the goldens.","title":"Regression Guardrails"},{"location":"RELEASE_STATUS.html","text":"didimpute v0.1.0 \u2014 Release Status Preflight Results (2025-10-14) \u2705 .venv/Scripts/python.exe -m ruff check \u2705 .venv/Scripts/python.exe -m mypy --strict src \u2705 .venv/Scripts/python.exe -m pytest -q --cov --cov-report=term \u2192 total coverage 91% (modules api 77%, utils 69% need extra tests) \u2705 .venv/Scripts/python.exe -m mkdocs build --strict (nav enabled via new mkdocs.yml ) Documentation Assessment Strengths: README offers concise API/CLI snippets; SPEC and PARITY docs outline estimator behavior and validation harness; ROADMAP/DECISIONS enumerate scope and future work. Gaps blocking release polish: README lacks installation guidance ( pip install , supported Python versions), license synopsis, citation of the original DID research, release DOI/contact, and guidance on reproducibility expectations. No contributor-facing or user-facing instructions for rerunning the parity harness (Stata dependency, expected environment variables, seed handling, parity/out artifacts). ARCHITECTURE.md is too terse for new contributors (needs component responsibilities, data flow, and error propagation details). Docs do not describe data generation scripts, deterministic seeds, or how to reproduce coverage/parity claims; add a reproducibility checklist. Missing citation or acknowledgement file (e.g., CITATION.cff ) and cross-links from docs to LICENSE and TEST_STRATEGY. Packaging & Metadata pyproject.toml omits PyPI classifiers (Python versions, intended audience, license), project URLs (homepage/docs/source/issues), and keywords; add before release. Hatch build config is absent, so the sdist currently bundles parity logs, CSV outputs, and upstream Stata sources ( parity/out/* , *_stata.log , tmp_old.txt , etc.). Prune these via tool.hatch.build (or MANIFEST) to keep the distribution lean and avoid machine-specific artifacts. Confirm whether shipping GPL-licensed Stata sources inside upstream/git/ is necessary; if retained, document licensing segregation and ensure compliance (dual-license notice, attribution in README/docs). Validate that wheel metadata exposes long description/content type (OK) and add maintainer info if applicable. Ethical & Reproducibility Review Document synthetic data generation, random-seed policy (currently seeds 100..149), and how to recreate regression goldens. Clarify external tooling requirements (e.g., Stata 18 path) and expected environment variables in parity documentation to ensure reproducibility without leaking machine-specific paths. State explicitly that no proprietary datasets/code are shipped; ensure parity outputs do not embed licensed Stata material beyond what\u2019s permitted. Repository Hygiene Remove or ignore locally generated logs and parity outputs ( *_stata.log , peek*.log , parity/out/*.csv , _tmp.log , tmp_old.txt , debug.log ) before tagging the release. Drop built artifacts ( dist/ ) from version control; rely on CI/CLI build steps instead. Ensure .venv/ stays untracked and consider adding docs/site/ (mkdocs output) to .gitignore if publishing docs separately. Add automated audit (pre-commit or CI) to prevent large binaries/logs from entering future dists. Recommendation \u26a0\ufe0f Needs polish before public release. Prioritize: (1) documentation refresh (installation, licensing, parity reproducibility, architecture detail, citation/contact), (2) packaging cleanup (metadata + sdist pruning + license clarification), (3) hygiene sweep to drop logs/artifacts and add guardrails, (4) bolster tests around low-covered modules ( api , utils ).","title":"didimpute v0.1.0 \u2014 Release Status"},{"location":"RELEASE_STATUS.html#didimpute-v010-release-status","text":"","title":"didimpute v0.1.0 \u2014 Release Status"},{"location":"RELEASE_STATUS.html#preflight-results-2025-10-14","text":"\u2705 .venv/Scripts/python.exe -m ruff check \u2705 .venv/Scripts/python.exe -m mypy --strict src \u2705 .venv/Scripts/python.exe -m pytest -q --cov --cov-report=term \u2192 total coverage 91% (modules api 77%, utils 69% need extra tests) \u2705 .venv/Scripts/python.exe -m mkdocs build --strict (nav enabled via new mkdocs.yml )","title":"Preflight Results (2025-10-14)"},{"location":"RELEASE_STATUS.html#documentation-assessment","text":"Strengths: README offers concise API/CLI snippets; SPEC and PARITY docs outline estimator behavior and validation harness; ROADMAP/DECISIONS enumerate scope and future work. Gaps blocking release polish: README lacks installation guidance ( pip install , supported Python versions), license synopsis, citation of the original DID research, release DOI/contact, and guidance on reproducibility expectations. No contributor-facing or user-facing instructions for rerunning the parity harness (Stata dependency, expected environment variables, seed handling, parity/out artifacts). ARCHITECTURE.md is too terse for new contributors (needs component responsibilities, data flow, and error propagation details). Docs do not describe data generation scripts, deterministic seeds, or how to reproduce coverage/parity claims; add a reproducibility checklist. Missing citation or acknowledgement file (e.g., CITATION.cff ) and cross-links from docs to LICENSE and TEST_STRATEGY.","title":"Documentation Assessment"},{"location":"RELEASE_STATUS.html#packaging-metadata","text":"pyproject.toml omits PyPI classifiers (Python versions, intended audience, license), project URLs (homepage/docs/source/issues), and keywords; add before release. Hatch build config is absent, so the sdist currently bundles parity logs, CSV outputs, and upstream Stata sources ( parity/out/* , *_stata.log , tmp_old.txt , etc.). Prune these via tool.hatch.build (or MANIFEST) to keep the distribution lean and avoid machine-specific artifacts. Confirm whether shipping GPL-licensed Stata sources inside upstream/git/ is necessary; if retained, document licensing segregation and ensure compliance (dual-license notice, attribution in README/docs). Validate that wheel metadata exposes long description/content type (OK) and add maintainer info if applicable.","title":"Packaging &amp; Metadata"},{"location":"RELEASE_STATUS.html#ethical-reproducibility-review","text":"Document synthetic data generation, random-seed policy (currently seeds 100..149), and how to recreate regression goldens. Clarify external tooling requirements (e.g., Stata 18 path) and expected environment variables in parity documentation to ensure reproducibility without leaking machine-specific paths. State explicitly that no proprietary datasets/code are shipped; ensure parity outputs do not embed licensed Stata material beyond what\u2019s permitted.","title":"Ethical &amp; Reproducibility Review"},{"location":"RELEASE_STATUS.html#repository-hygiene","text":"Remove or ignore locally generated logs and parity outputs ( *_stata.log , peek*.log , parity/out/*.csv , _tmp.log , tmp_old.txt , debug.log ) before tagging the release. Drop built artifacts ( dist/ ) from version control; rely on CI/CLI build steps instead. Ensure .venv/ stays untracked and consider adding docs/site/ (mkdocs output) to .gitignore if publishing docs separately. Add automated audit (pre-commit or CI) to prevent large binaries/logs from entering future dists.","title":"Repository Hygiene"},{"location":"RELEASE_STATUS.html#recommendation","text":"\u26a0\ufe0f Needs polish before public release. Prioritize: (1) documentation refresh (installation, licensing, parity reproducibility, architecture detail, citation/contact), (2) packaging cleanup (metadata + sdist pruning + license clarification), (3) hygiene sweep to drop logs/artifacts and add guardrails, (4) bolster tests around low-covered modules ( api , utils ).","title":"Recommendation"},{"location":"SPEC.html","text":"SPEC \u2014 Behavioral Matrix (v0.1) Input columns - outcome: y; unit: id; time: t (integer-castable); adoption time: Ei (NaN => never-treated) - optional: controls X[], weight w>=0, cluster id (defaults to id) Estimator steps 1) Untreated sample = never-treated OR not-yet-treated (t < Ei). 2) Two-way FE on untreated sample: y ~ controls + \u03b1_i + \u03bb_t (WLS if weights). 3) Predict counterfactual for all cells with observed (i,t) labels from untreated sample; compute: - treated-post cells: \u03c4_it = y_it - \u0177^0_it - eventual-treated pre-periods: placebo for pretrend test 4) Aggregate \u03c4_it to event-time k = t - Ei with selected weights. 5) Cluster-robust SEs by id for each k; joint pretrend test over k<0. Aggregation weights (per event-time k) - nobs: equal weight per cell (simple mean across treated cells at k). - equal: equal weight per cohort (cohort = Ei); average cohort means at k equally. - cohort_share: weight cohort means by cohort share at k (n_{g,k}/N_k). CLI - didimpute --csv ... --y Y --id i --time t --Ei Ei [--cluster id --controls X1,X2 --weight w --horizons -5:10 --pretrends 5 --scheme equal|nobs|cohort_share] Limitations v0.1 - Single adoption event; no wild bootstrap; one-way clustering; no missing-data strategies beyond listwise.","title":"SPEC \u2014 Behavioral Matrix (v0.1)"},{"location":"SPEC.html#spec-behavioral-matrix-v01","text":"Input columns - outcome: y; unit: id; time: t (integer-castable); adoption time: Ei (NaN => never-treated) - optional: controls X[], weight w>=0, cluster id (defaults to id) Estimator steps 1) Untreated sample = never-treated OR not-yet-treated (t < Ei). 2) Two-way FE on untreated sample: y ~ controls + \u03b1_i + \u03bb_t (WLS if weights). 3) Predict counterfactual for all cells with observed (i,t) labels from untreated sample; compute: - treated-post cells: \u03c4_it = y_it - \u0177^0_it - eventual-treated pre-periods: placebo for pretrend test 4) Aggregate \u03c4_it to event-time k = t - Ei with selected weights. 5) Cluster-robust SEs by id for each k; joint pretrend test over k<0. Aggregation weights (per event-time k) - nobs: equal weight per cell (simple mean across treated cells at k). - equal: equal weight per cohort (cohort = Ei); average cohort means at k equally. - cohort_share: weight cohort means by cohort share at k (n_{g,k}/N_k). CLI - didimpute --csv ... --y Y --id i --time t --Ei Ei [--cluster id --controls X1,X2 --weight w --horizons -5:10 --pretrends 5 --scheme equal|nobs|cohort_share] Limitations v0.1 - Single adoption event; no wild bootstrap; one-way clustering; no missing-data strategies beyond listwise.","title":"SPEC \u2014 Behavioral Matrix (v0.1)"}]}